{"cells":[{"cell_type":"code","execution_count":13,"metadata":{"id":"E9r2_CdEEvHm"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","import boto3\n","from dotenv import load_dotenv\n","import os\n","\n","# Carregar variáveis de ambiente do arquivo .env\n","load_dotenv()\n","\n","# Obter as chaves da AWS das variáveis de ambiente\n","aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n","aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n","\n","# Iniciando uma SparkSession com Delta Lake\n","spark = SparkSession.builder \\\n","    .appName(\"DeltaLakeSparkS3\") \\\n","    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n","    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n","    .getOrCreate()\n","\n","# Adicionando configurações S3 diretamente na sessão Spark\n","spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", aws_access_key_id)\n","spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", aws_secret_access_key)\n","spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n","spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n","\n","# Configurações dos buckets S3\n","landing_bucket = \"engenharia-dados-satc-landing-zone-bucket\"\n","bronze_bucket = \"engenharia-dados-satc-bronze-bucket\"\n","\n","# Verificando os arquivos no bucket S3\n","s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n","response = s3.list_objects_v2(Bucket=landing_bucket)\n","\n","print(\"Arquivos no bucket S3:\")\n","if 'Contents' in response:\n","    for obj in response['Contents']:\n","        print(obj['Key'])\n","else:\n","    print(\"Nenhum arquivo encontrado no bucket especificado.\")\n","\n","# Função para ler dados do S3\n","def read_from_s3(bucket, path):\n","    try:\n","        df = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"s3a://{bucket}/{path}\")\n","        return df\n","    except Exception as e:\n","        print(f\"Erro ao ler dados do S3: {e}\")\n","\n","# Função principal\n","def main():\n","    try:\n","        # Definindo os caminhos dos arquivos CSV na landing\n","        paths = ['geo.csv', 'dadosprodutos.csv', 'pedidos.csv', 'clientes.csv', 'vendedores.csv', 'pagamentos.csv', 'dadospedidos.csv']\n","        dfs = {}\n","        for path in paths:\n","            df_name = path.split('.')[0]\n","            dfs[df_name] = read_from_s3(landing_bucket, f\"/{path}\")\n","\n","        # Salvando os DataFrames no formato Parquet usando Delta Lake no bucket da camada bronze\n","        for df_name, df in dfs.items():\n","            df.write.format(\"delta\").mode('overwrite').save(f\"s3a://{bronze_bucket}/{df_name}\")\n","        \n","        print(\"Dados salvos na camada bronze com sucesso.\")\n","        \n","    except Exception as e:\n","        print(f\"Erro no processo principal: {e}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNS3LZVuZr2jDHv8qE0Axcp","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
