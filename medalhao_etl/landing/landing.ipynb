{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2448ade2-7ca5-4a93-8424-fc82cd312dda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos no bucket S3:\nclientes.csv/_SUCCESS\nclientes.csv/_committed_9112049085076660698\nclientes.csv/_started_9112049085076660698\nclientes.csv/part-00000-tid-9112049085076660698-6d781d67-96b0-4965-93d1-6404a6992252-1-1-c000.csv\ndadospedidos.csv/_SUCCESS\ndadospedidos.csv/_committed_2411623796930298435\ndadospedidos.csv/_started_2411623796930298435\ndadospedidos.csv/part-00000-tid-2411623796930298435-5e58a814-021a-4f6a-988d-8f0047b6d786-4-1-c000.csv\ndadosprodutos.csv/_SUCCESS\ndadosprodutos.csv/_committed_5449724296839749973\ndadosprodutos.csv/_started_5449724296839749973\ndadosprodutos.csv/part-00000-tid-5449724296839749973-cb4154ad-ea1b-4619-a844-031207ee518c-6-1-c000.csv\ngeo.csv/_SUCCESS\ngeo.csv/_committed_8630313209848014780\ngeo.csv/_started_8630313209848014780\ngeo.csv/part-00000-tid-8630313209848014780-0f3f4cfd-9f37-47a0-8321-f8c8c2209ff2-0-1-c000.csv\npagamentos.csv/_SUCCESS\npagamentos.csv/_committed_5853869466763043177\npagamentos.csv/_started_5853869466763043177\npagamentos.csv/part-00000-tid-5853869466763043177-72e56667-8daf-4410-b138-eac0b1ebfa68-5-1-c000.csv\npedidos.csv/_SUCCESS\npedidos.csv/_committed_3710389847656272558\npedidos.csv/_started_3710389847656272558\npedidos.csv/part-00000-tid-3710389847656272558-3fd68313-769d-4141-b299-2cb09be8c87d-3-1-c000.csv\nvendedores.csv/_SUCCESS\nvendedores.csv/_committed_2408650268114015146\nvendedores.csv/_started_2408650268114015146\nvendedores.csv/part-00000-tid-2408650268114015146-7b770157-3e91-4f72-84f1-c692199021e6-2-1-c000.csv\nDados salvos na camada bronze com sucesso.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "# Iniciando uma SparkSession com Delta Lake\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeSparkS3\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Adicionando configurações S3 diretamente na sessão Spark\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY\"))\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", os.getenv(\"AWS_SECRET_KEY\"))\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "# Configurações dos buckets S3\n",
    "landing_bucket = \"engenharia-dados-satc-landing-zone-bucket\"\n",
    "bronze_bucket = \"engenharia-dados-satc-bronze-bucket\"\n",
    "\n",
    "# Verificando os arquivos no bucket S3\n",
    "s3 = boto3.client('s3', aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY\"), aws_secret_access_key=os.getenv(\"AWS_SECRET_KEY\"))\n",
    "response = s3.list_objects_v2(Bucket=landing_bucket)\n",
    "\n",
    "print(\"Arquivos no bucket S3:\")\n",
    "if 'Contents' in response:\n",
    "    for obj in response['Contents']:\n",
    "        print(obj['Key'])\n",
    "else:\n",
    "    print(\"Nenhum arquivo encontrado no bucket especificado.\")\n",
    "\n",
    "# Função para ler dados do S3\n",
    "def read_from_s3(bucket, path):\n",
    "    try:\n",
    "        df = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"s3a://{bucket}/{path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler dados do S3: {e}\")\n",
    "\n",
    "# Função principal\n",
    "def main():\n",
    "    try:\n",
    "        # Definindo os caminhos dos arquivos CSV na landing\n",
    "        paths = ['dadosprodutos.csv', 'pedidos.csv', 'clientes.csv', 'vendedores.csv', 'pagamentos.csv', 'dadospedidos.csv']\n",
    "        dfs = {}\n",
    "        for path in paths:\n",
    "            df_name = path.split('.')[0]\n",
    "            dfs[df_name] = read_from_s3(landing_bucket, f\"/{path}\")\n",
    "        \n",
    "        dfs[\"localizacoes\"] = read_from_s3(landing_bucket, f\"/geo.csv\")\n",
    "\n",
    "        # Salvando os DataFrames no formato Parquet usando Delta Lake no bucket da camada bronze\n",
    "        for df_name, df in dfs.items():\n",
    "            df.write.format(\"delta\").mode('overwrite').save(f\"s3a://{bronze_bucket}/{df_name}\")\n",
    "        \n",
    "        print(\"Dados salvos na camada bronze com sucesso.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro no processo principal: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "landing",
   "widgets": {}
  },
  "colab": {
   "authorship_tag": "ABX9TyNS3LZVuZr2jDHv8qE0Axcp",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
