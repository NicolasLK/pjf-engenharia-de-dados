{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Projeto Engenharia de Dados Trabalho desenvolvido para a disciplina de Engenharia de dados do Curso de Engenharia da Software da UNISATC. A proposta do projeto e desenvolver uma pipeline de engenharia de dados... Objetivo do projeto O objetivo deste projeto \u00e9 desenvolver um pipeline de dados automatizado que coleta, processa e torne possivel a analise deles. Os dados originais s\u00e3o do seguinte Dataset , para ser possivel utiliza-los, foi necessario armazena-los em um banco relacional. Utilizaremos esses dados para criar as seguintes analises: Vendas : Ser\u00e1 informado o volume, vendedores, valor medio do frete, valor total do frete e etc; Entregas : Ser\u00e1 informado o tempo m\u00e9dio em dias, volume m\u00e9dio produtos, volume total produtos, massa m\u00e9dia produtos e massa total produtos. Integrantes Henrique Angar - DESENVOLVIMENTO Joel Francisco - DESENVOLVIMENTO Juliano Felipe - DOCUMENTA\u00c7\u00c3O Lorenzo Dal B\u00f3 - DOCUMENTA\u00c7\u00c3O El\u00f3i Matos - DESENVOLVIMENTO Nicolas L. Kaminski - DOCUMENTA\u00c7\u00c3O Rafael Castro - DOCUMENTA\u00c7\u00c3O Yuri Boppre - DESENVOLVIMENTO T\u00e9cnologias utilizadas Amazon S3 : Servi\u00e7o de armazenamento de objetos; Apache airFlow : Plataforma para criar, agendar e monitorar fluxos de trabalhos; Apache Spark : Engine de processamento de dados distribuidos; Draw.io : Software de desenho gr\u00e1fico multiplataforma, utilizado para criar diagramas como fluxogramas, wireframes entre outros; Microsoft Power BI : Servi\u00e7o de an\u00e1lise de neg\u00f3cios e analise de dados; MKDocs : Gerador de sites est\u00e1ticos com foco na montagem em documenta\u00e7\u00e3o de projetos; PostgreSQL : Poderoso banco de dados relacional de c\u00f3digo aberto; Python : Linguagem de programa\u00e7\u00e3o de alto n\u00edvel, interpretada de script. Databricks : Databricks \u00e9 uma plataforma para trabalhar com o Spark, que fornece gerenciamento automatizado de cluster e notebooks FigJam Board : \u00e9 um quadro branco para que equipes possam realizar sess\u00f5es de idea\u00e7\u00e3o e brainstorming juntos, de forma on-line","title":"Home"},{"location":"#projeto-engenharia-de-dados","text":"Trabalho desenvolvido para a disciplina de Engenharia de dados do Curso de Engenharia da Software da UNISATC. A proposta do projeto e desenvolver uma pipeline de engenharia de dados...","title":"Projeto Engenharia de Dados"},{"location":"#objetivo-do-projeto","text":"O objetivo deste projeto \u00e9 desenvolver um pipeline de dados automatizado que coleta, processa e torne possivel a analise deles. Os dados originais s\u00e3o do seguinte Dataset , para ser possivel utiliza-los, foi necessario armazena-los em um banco relacional. Utilizaremos esses dados para criar as seguintes analises: Vendas : Ser\u00e1 informado o volume, vendedores, valor medio do frete, valor total do frete e etc; Entregas : Ser\u00e1 informado o tempo m\u00e9dio em dias, volume m\u00e9dio produtos, volume total produtos, massa m\u00e9dia produtos e massa total produtos.","title":"Objetivo do projeto"},{"location":"#integrantes","text":"Henrique Angar - DESENVOLVIMENTO Joel Francisco - DESENVOLVIMENTO Juliano Felipe - DOCUMENTA\u00c7\u00c3O Lorenzo Dal B\u00f3 - DOCUMENTA\u00c7\u00c3O El\u00f3i Matos - DESENVOLVIMENTO Nicolas L. Kaminski - DOCUMENTA\u00c7\u00c3O Rafael Castro - DOCUMENTA\u00c7\u00c3O Yuri Boppre - DESENVOLVIMENTO","title":"Integrantes"},{"location":"#tecnologias-utilizadas","text":"Amazon S3 : Servi\u00e7o de armazenamento de objetos; Apache airFlow : Plataforma para criar, agendar e monitorar fluxos de trabalhos; Apache Spark : Engine de processamento de dados distribuidos; Draw.io : Software de desenho gr\u00e1fico multiplataforma, utilizado para criar diagramas como fluxogramas, wireframes entre outros; Microsoft Power BI : Servi\u00e7o de an\u00e1lise de neg\u00f3cios e analise de dados; MKDocs : Gerador de sites est\u00e1ticos com foco na montagem em documenta\u00e7\u00e3o de projetos; PostgreSQL : Poderoso banco de dados relacional de c\u00f3digo aberto; Python : Linguagem de programa\u00e7\u00e3o de alto n\u00edvel, interpretada de script. Databricks : Databricks \u00e9 uma plataforma para trabalhar com o Spark, que fornece gerenciamento automatizado de cluster e notebooks FigJam Board : \u00e9 um quadro branco para que equipes possam realizar sess\u00f5es de idea\u00e7\u00e3o e brainstorming juntos, de forma on-line","title":"T\u00e9cnologias utilizadas"},{"location":"about/","text":"Sobre o Projeto de Pipeline Constru\u00e7\u00e3o do Projeto 1. Coleta de Dados Como mencionado na pagina Home utilizamos o seguinte Dataset para manipula\u00e7\u00e3o de dados, segue o diagrama relacional: Utilizamos o airflow para iniciar a coleta dos dados de um banco relacional, com objetivo de popular a camada de Landing . Codigo aqui 2. Processamento de Dados O processamento segue o seguinte fluxo: Come\u00e7ando pela camada Landing: Camada Landing Com o c\u00f3digo abaixo iniciamos uma sess\u00e3o Spark utilizando Delta Lake. # Iniciando uma SparkSession com Delta Lake spark = SparkSession.builder \\ .appName(\"DeltaLakeSparkS3\") \\ .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\ .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\ .getOrCreate() Como comentado na Home estamos utilizando Amazon S3 . # Fun\u00e7\u00e3o para ler dados do S3 def read_from_s3(bucket, path): try: df = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"s3a://{bucket}/{path}\") return df except Exception as e: print(f\"Erro ao ler dados do S3: {e}\") Depois de realizado a leitura, salvamos os dados em formato PARQUET e direcionamos os dados para a camada Bronze . # Fun\u00e7\u00e3o principal def main(): try: # Definindo os caminhos dos arquivos CSV na landing paths = ['geo.csv', 'dadosprodutos.csv', 'pedidos.csv', 'clientes.csv', 'vendedores.csv', 'pagamentos.csv', 'dadospedidos.csv'] dfs = {} for path in paths: df_name = path.split('.')[0] dfs[df_name] = read_from_s3(landing_bucket, f\"/{path}\") # Salvando os DataFrames no formato Parquet usando Delta Lake no bucket da camada bronze for df_name, df in dfs.items(): df.write.format(\"delta\").mode('overwrite').save(f\"s3a://{bronze_bucket}/{df_name}\") print(\"Dados salvos na camada bronze com sucesso.\") except Exception as e: print(f\"Erro no processo principal: {e}\") if __name__ == \"__main__\": main() Camada Bronze Agora os dados se encontram na camada Bronze . Como iniciamos outro processo \u00e9 necessario iniciar outra sess\u00e3o Spark. # Iniciando uma SparkSession com Delta Lake spark = SparkSession.builder \\ .appName(\"BronzeLayer\") \\ .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\ .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\ .getOrCreate() Agora carregamos os dados para a camada Bronze . # Carregar arquivos Delta da camada bronze localizacoes = spark.read.format(\"delta\").load(f'{bronze_bucket}/localizacoes') produtos = spark.read.format(\"delta\").load(f'{bronze_bucket}/dadosprodutos') pedidos = spark.read.format(\"delta\").load(f'{bronze_bucket}/pedidos') clientes = spark.read.format(\"delta\").load(f'{bronze_bucket}/clientes') vendedores = spark.read.format(\"delta\").load(f'{bronze_bucket}/vendedores') pagamentos = spark.read.format(\"delta\").load(f'{bronze_bucket}/pagamentos') dadospedidos = spark.read.format(\"delta\").load(f'{bronze_bucket}/dadospedidos') Renomeamos as colunas. # Renomear colunas clientes = clientes.withColumnRenamed('idgeralcliente', 'idcliente') produtos = produtos.withColumnRenamed('id_produto', 'idproduto') Cria\u00e7\u00e3o da tabela dimensional DIM_DATA e utilizando ela para criar os DataFrames . # CRIA DIM_DATA # Gerar uma lista de datas de 2015 at\u00e9 hoje + 1 dia end_date = datetime.now() + timedelta(days=1) dates = [end_date - timedelta(days=x) for x in range((end_date - datetime(2015, 1, 1)).days + 1)] # Criar um DataFrame com as datas schema = StructType([ StructField('dt_data', StringType(), True) ]) df = spark.createDataFrame([(d.strftime('%Y-%m-%d'),) for d in dates], schema=schema) df = df.withColumn('dt_diasemana', dayofweek(col('dt_data'))) df = df.withColumn('dt_diasemana_ord', dayofweek(col('dt_data'))) df = df.withColumn('dt_mesano', col('dt_data').substr(0, 7)) df = df.withColumn('dt_mesano_ord', (year(col('dt_data')) - 2015) * 12 + month(col('dt_data'))) df = df.withColumn('dt_trimestre', quarter(col('dt_data'))) df = df.withColumn('dt_trimestre_ord', (year(col('dt_data')) - 2015) * 4 + quarter(col('dt_data'))) df = df.withColumn('dt_ano', year(col('dt_data'))) df = df.withColumn('sk_data', year(col('dt_data'))) Necessario reorganizar as colunas para o formato especificado . # Reorganizar as colunas conforme o formato especificado datas = df.select('sk_data', 'dt_data', 'dt_diasemana', 'dt_diasemana_ord', 'dt_mesano', 'dt_mesano_ord', 'dt_trimestre', 'dt_trimestre_ord', 'dt_ano') Cria\u00e7\u00e3o da tabela dimensional DIM_PEDIDOS . Logo em seguida salvamos na camada Silver . ### CRIA PEDIDOS ### temp = pedidos.join(dadospedidos, on='idpedido', how='left') \\ .join(pagamentos, on='idpedido', how='left') \\ .join(vendedores, on='idvendedor', how='left') \\ .join(clientes, on='idcliente', how='left') \\ .join(produtos, on='idproduto', how='left') temp.show() temp.filter(temp['cepcliente'].isNotNull()).show() ### SALVAR TABELAS NA CAMADA SILVER ### temp.write.format(\"delta\").mode('overwrite').save(f'{silver_bucket}/pedidos') localizacoes.write.format(\"delta\").mode('overwrite').save(f'{silver_bucket}/localizacoes') datas.write.format(\"delta\").mode('overwrite').save(f'{silver_bucket}/dim_data') 3. Automa\u00e7\u00e3o O carregamento de dados na pipeline \u00e9 realizado automaticamente por meio do m\u00e9todo airflow. Explica\u00e7\u00e3o: Exeplo: Codigo aqui","title":"About"},{"location":"about/#sobre-o-projeto-de-pipeline","text":"","title":"Sobre o Projeto de Pipeline"},{"location":"about/#construcao-do-projeto","text":"","title":"Constru\u00e7\u00e3o do Projeto"},{"location":"about/#1-coleta-de-dados","text":"Como mencionado na pagina Home utilizamos o seguinte Dataset para manipula\u00e7\u00e3o de dados, segue o diagrama relacional: Utilizamos o airflow para iniciar a coleta dos dados de um banco relacional, com objetivo de popular a camada de Landing . Codigo aqui","title":"1. Coleta de Dados"},{"location":"about/#2-processamento-de-dados","text":"O processamento segue o seguinte fluxo: Come\u00e7ando pela camada Landing:","title":"2. Processamento de Dados"},{"location":"about/#camada-landing","text":"Com o c\u00f3digo abaixo iniciamos uma sess\u00e3o Spark utilizando Delta Lake. # Iniciando uma SparkSession com Delta Lake spark = SparkSession.builder \\ .appName(\"DeltaLakeSparkS3\") \\ .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\ .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\ .getOrCreate() Como comentado na Home estamos utilizando Amazon S3 . # Fun\u00e7\u00e3o para ler dados do S3 def read_from_s3(bucket, path): try: df = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"s3a://{bucket}/{path}\") return df except Exception as e: print(f\"Erro ao ler dados do S3: {e}\") Depois de realizado a leitura, salvamos os dados em formato PARQUET e direcionamos os dados para a camada Bronze . # Fun\u00e7\u00e3o principal def main(): try: # Definindo os caminhos dos arquivos CSV na landing paths = ['geo.csv', 'dadosprodutos.csv', 'pedidos.csv', 'clientes.csv', 'vendedores.csv', 'pagamentos.csv', 'dadospedidos.csv'] dfs = {} for path in paths: df_name = path.split('.')[0] dfs[df_name] = read_from_s3(landing_bucket, f\"/{path}\") # Salvando os DataFrames no formato Parquet usando Delta Lake no bucket da camada bronze for df_name, df in dfs.items(): df.write.format(\"delta\").mode('overwrite').save(f\"s3a://{bronze_bucket}/{df_name}\") print(\"Dados salvos na camada bronze com sucesso.\") except Exception as e: print(f\"Erro no processo principal: {e}\") if __name__ == \"__main__\": main()","title":"Camada Landing"},{"location":"about/#camada-bronze","text":"Agora os dados se encontram na camada Bronze . Como iniciamos outro processo \u00e9 necessario iniciar outra sess\u00e3o Spark. # Iniciando uma SparkSession com Delta Lake spark = SparkSession.builder \\ .appName(\"BronzeLayer\") \\ .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\ .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\ .getOrCreate() Agora carregamos os dados para a camada Bronze . # Carregar arquivos Delta da camada bronze localizacoes = spark.read.format(\"delta\").load(f'{bronze_bucket}/localizacoes') produtos = spark.read.format(\"delta\").load(f'{bronze_bucket}/dadosprodutos') pedidos = spark.read.format(\"delta\").load(f'{bronze_bucket}/pedidos') clientes = spark.read.format(\"delta\").load(f'{bronze_bucket}/clientes') vendedores = spark.read.format(\"delta\").load(f'{bronze_bucket}/vendedores') pagamentos = spark.read.format(\"delta\").load(f'{bronze_bucket}/pagamentos') dadospedidos = spark.read.format(\"delta\").load(f'{bronze_bucket}/dadospedidos') Renomeamos as colunas. # Renomear colunas clientes = clientes.withColumnRenamed('idgeralcliente', 'idcliente') produtos = produtos.withColumnRenamed('id_produto', 'idproduto') Cria\u00e7\u00e3o da tabela dimensional DIM_DATA e utilizando ela para criar os DataFrames . # CRIA DIM_DATA # Gerar uma lista de datas de 2015 at\u00e9 hoje + 1 dia end_date = datetime.now() + timedelta(days=1) dates = [end_date - timedelta(days=x) for x in range((end_date - datetime(2015, 1, 1)).days + 1)] # Criar um DataFrame com as datas schema = StructType([ StructField('dt_data', StringType(), True) ]) df = spark.createDataFrame([(d.strftime('%Y-%m-%d'),) for d in dates], schema=schema) df = df.withColumn('dt_diasemana', dayofweek(col('dt_data'))) df = df.withColumn('dt_diasemana_ord', dayofweek(col('dt_data'))) df = df.withColumn('dt_mesano', col('dt_data').substr(0, 7)) df = df.withColumn('dt_mesano_ord', (year(col('dt_data')) - 2015) * 12 + month(col('dt_data'))) df = df.withColumn('dt_trimestre', quarter(col('dt_data'))) df = df.withColumn('dt_trimestre_ord', (year(col('dt_data')) - 2015) * 4 + quarter(col('dt_data'))) df = df.withColumn('dt_ano', year(col('dt_data'))) df = df.withColumn('sk_data', year(col('dt_data'))) Necessario reorganizar as colunas para o formato especificado . # Reorganizar as colunas conforme o formato especificado datas = df.select('sk_data', 'dt_data', 'dt_diasemana', 'dt_diasemana_ord', 'dt_mesano', 'dt_mesano_ord', 'dt_trimestre', 'dt_trimestre_ord', 'dt_ano') Cria\u00e7\u00e3o da tabela dimensional DIM_PEDIDOS . Logo em seguida salvamos na camada Silver . ### CRIA PEDIDOS ### temp = pedidos.join(dadospedidos, on='idpedido', how='left') \\ .join(pagamentos, on='idpedido', how='left') \\ .join(vendedores, on='idvendedor', how='left') \\ .join(clientes, on='idcliente', how='left') \\ .join(produtos, on='idproduto', how='left') temp.show() temp.filter(temp['cepcliente'].isNotNull()).show() ### SALVAR TABELAS NA CAMADA SILVER ### temp.write.format(\"delta\").mode('overwrite').save(f'{silver_bucket}/pedidos') localizacoes.write.format(\"delta\").mode('overwrite').save(f'{silver_bucket}/localizacoes') datas.write.format(\"delta\").mode('overwrite').save(f'{silver_bucket}/dim_data')","title":"Camada Bronze"},{"location":"about/#3-automacao","text":"O carregamento de dados na pipeline \u00e9 realizado automaticamente por meio do m\u00e9todo airflow. Explica\u00e7\u00e3o: Exeplo: Codigo aqui","title":"3. Automa\u00e7\u00e3o"}]}